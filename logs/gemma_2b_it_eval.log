nohup: ignoring input
/home/eyuboglu@stanford.edu/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/eyuboglu@stanford.edu/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
2024-04-30:20:33:12,117 INFO     [__main__.py:206] Verbosity set to INFO
2024-04-30:20:33:12,117 INFO     [__init__.py:358] lm_eval.tasks.initialize_tasks() is deprecated and no longer necessary. It will be removed in v0.4.2 release. TaskManager will instead be used.
2024-04-30:20:33:16,979 INFO     [__main__.py:282] Selected Tasks: ['boolq', 'cb', 'copa', 'multirc', 'wic', 'wsc']
2024-04-30:20:33:16,979 INFO     [__main__.py:283] Loading selected tasks...
2024-04-30:20:33:16,981 INFO     [evaluator.py:95] Setting random seed to 0
2024-04-30:20:33:16,981 INFO     [evaluator.py:99] Setting numpy seed to 1234
2024-04-30:20:33:16,981 INFO     [evaluator.py:103] Setting torch manual seed to 1234
2024-04-30:20:33:17,611 WARNING  [logging.py:60] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2024-04-30:20:33:17,612 INFO     [huggingface.py:161] Using device 'cuda:0'
Traceback (most recent call last):
  File "/home/eyuboglu@stanford.edu/.local/bin/lm_eval", line 8, in <module>
    sys.exit(cli_evaluate())
  File "/var/cr05_data/sabri/jonsf/PretrainingProject/PretrainingObjectives/pretraining/olive-evaluation-harness/lm_eval/__main__.py", line 285, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/var/cr05_data/sabri/jonsf/PretrainingProject/PretrainingObjectives/pretraining/olive-evaluation-harness/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/var/cr05_data/sabri/jonsf/PretrainingProject/PretrainingObjectives/pretraining/olive-evaluation-harness/lm_eval/evaluator.py", line 123, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/var/cr05_data/sabri/jonsf/PretrainingProject/PretrainingObjectives/pretraining/olive-evaluation-harness/lm_eval/api/model.py", line 134, in create_from_arg_string
    return cls(**args, **args2)
  File "/var/cr05_data/sabri/jonsf/PretrainingProject/PretrainingObjectives/pretraining/olive-evaluation-harness/lm_eval/models/huggingface.py", line 187, in __init__
    self._get_config(
  File "/var/cr05_data/sabri/jonsf/PretrainingProject/PretrainingObjectives/pretraining/olive-evaluation-harness/lm_eval/models/huggingface.py", line 444, in _get_config
    self._config = transformers.AutoConfig.from_pretrained(
  File "/home/eyuboglu@stanford.edu/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1050, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/home/eyuboglu@stanford.edu/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 748, in __getitem__
    raise KeyError(key)
KeyError: 'gemma'
