nohup: ignoring input
2024-04-25:02:33:18,741 INFO     [__main__.py:206] Verbosity set to INFO
2024-04-25:02:33:18,741 INFO     [__init__.py:358] lm_eval.tasks.initialize_tasks() is deprecated and no longer necessary. It will be removed in v0.4.2 release. TaskManager will instead be used.
2024-04-25:02:33:23,518 INFO     [__main__.py:282] Selected Tasks: ['code2text_java', 'code2text_python']
2024-04-25:02:33:23,518 INFO     [__main__.py:283] Loading selected tasks...
2024-04-25:02:33:23,520 INFO     [evaluator.py:95] Setting random seed to 0
2024-04-25:02:33:23,520 INFO     [evaluator.py:99] Setting numpy seed to 1234
2024-04-25:02:33:23,520 INFO     [evaluator.py:103] Setting torch manual seed to 1234
2024-04-25:02:33:24,706 WARNING  [logging.py:61] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2024-04-25:02:33:24,706 INFO     [huggingface.py:161] Using device 'cuda:0'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-04-25:02:33:27,720 INFO     [evaluator.py:150] get_task_dict has been updated to accept an optional argument, `task_manager`Read more here:https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/interface.md#external-library-usage
2024-04-25:02:34:49,967 INFO     [task.py:361] Building contexts for code2text_python on rank 0...
2024-04-25:02:34:52,833 INFO     [task.py:361] Building contexts for code2text_java on rank 0...
2024-04-25:02:34:57,786 INFO     [evaluator.py:373] Running generate_until requests
  0%|          | 0/25873 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/eyuboglu@stanford.edu/.local/bin/lm_eval", line 8, in <module>
    sys.exit(cli_evaluate())
  File "/var/cr05_data/sabri/jonsf/olive/olive-evaluation-harness/lm_eval/__main__.py", line 285, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/var/cr05_data/sabri/jonsf/olive/olive-evaluation-harness/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/var/cr05_data/sabri/jonsf/olive/olive-evaluation-harness/lm_eval/evaluator.py", line 190, in simple_evaluate
    results = evaluate(
  File "/var/cr05_data/sabri/jonsf/olive/olive-evaluation-harness/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/var/cr05_data/sabri/jonsf/olive/olive-evaluation-harness/lm_eval/evaluator.py", line 388, in evaluate
    resps = getattr(lm, reqtype)(cloned_reqs)
  File "/var/cr05_data/sabri/jonsf/olive/olive-evaluation-harness/lm_eval/models/huggingface.py", line 1161, in generate_until
    cont = self._model_generate(
  File "/var/cr05_data/sabri/jonsf/olive/olive-evaluation-harness/lm_eval/models/huggingface.py", line 754, in _model_generate
    return self.model.generate(
  File "/var/cr05_data/sim_data/miniconda3/envs/olive10.2/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/var/cr05_data/sim_data/miniconda3/envs/olive10.2/lib/python3.10/site-packages/transformers/generation/utils.py", line 1449, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/var/cr05_data/sim_data/miniconda3/envs/olive10.2/lib/python3.10/site-packages/transformers/generation/utils.py", line 1140, in _validate_generated_length
    raise ValueError(
ValueError: Input length of input_ids is 2088, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.
  0%|          | 0/25873 [00:19<?, ?it/s]
